{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_small_data = pd.read_csv('../data/processed/trim_small_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>Race and ethnicity (combined)</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>medcond_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Multiple/Other, Non-Hispanic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Multiple/Other, Non-Hispanic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex      age_group Race and ethnicity (combined) hosp_yn icu_yn  \\\n",
       "0  Female    0 - 9 Years  Multiple/Other, Non-Hispanic     Yes    Yes   \n",
       "1    Male    0 - 9 Years               Hispanic/Latino      No     No   \n",
       "2    Male    0 - 9 Years  Multiple/Other, Non-Hispanic      No     No   \n",
       "3    Male  10 - 19 Years               Hispanic/Latino     Yes     No   \n",
       "4  Female  10 - 19 Years               Hispanic/Latino      No     No   \n",
       "\n",
       "  death_yn medcond_yn  \n",
       "0       No        Yes  \n",
       "1       No         No  \n",
       "2       No         No  \n",
       "3       No        Yes  \n",
       "4       No         No  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_small_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim_small_data['hosp_yn'].replace('Yes',1, inplace=True)\n",
    "for columns in('hosp_yn','icu_yn','death_yn','medcond_yn','sex'):\n",
    "    trim_small_data[columns] = trim_small_data[columns].map({'Yes':1,'No':0,'Female':1,'Male':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>Race and ethnicity (combined)</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>medcond_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Multiple/Other, Non-Hispanic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Multiple/Other, Non-Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex      age_group Race and ethnicity (combined)  hosp_yn  icu_yn  \\\n",
       "0    1    0 - 9 Years  Multiple/Other, Non-Hispanic        1       1   \n",
       "1    0    0 - 9 Years               Hispanic/Latino        0       0   \n",
       "2    0    0 - 9 Years  Multiple/Other, Non-Hispanic        0       0   \n",
       "3    0  10 - 19 Years               Hispanic/Latino        1       0   \n",
       "4    1  10 - 19 Years               Hispanic/Latino        0       0   \n",
       "\n",
       "   death_yn  medcond_yn  \n",
       "0         0           1  \n",
       "1         0           0  \n",
       "2         0           0  \n",
       "3         0           1  \n",
       "4         0           0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_small_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_group = trim_small_data['age_group'].groupby(trim_small_data['age_group']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "0 - 9 Years       5564\n",
       "10 - 19 Years    18244\n",
       "20 - 29 Years    41221\n",
       "30 - 39 Years    33880\n",
       "40 - 49 Years    34433\n",
       "Name: age_group, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = trim_small_data['Race and ethnicity (combined)'].groupby(trim_small_data['Race and ethnicity (combined)']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Race and ethnicity (combined)\n",
       "American Indian/Alaska Native, Non-Hispanic      995\n",
       "Asian, Non-Hispanic                             8099\n",
       "Black, Non-Hispanic                            38499\n",
       "Hispanic/Latino                                56298\n",
       "Multiple/Other, Non-Hispanic                    7044\n",
       "Name: Race and ethnicity (combined), dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(titanic.drop(\"death_yn\",axis = 1))\n",
    "y = np.array(titanic[\"death_yn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bernoulli_Probability(data):\n",
    "    p = np.count_nonzero(data)/data.size\n",
    "    return ( lambda x: p if x > 0 else (1-p) )\n",
    "\n",
    "def Multinomial_Probability(data, smoothing=0):\n",
    "    values = data.value_counts()\n",
    "    total = values.sum()\n",
    "    N = values.nunique()\n",
    "    result = {i : (value + smoothing) / (total + smoothing * N) for i, value in values.items()}\n",
    "    return (lambda x: result[x])\n",
    "\n",
    "def Gaussian_Probability(data):\n",
    "    mu = data.mean()\n",
    "    std = data.std()\n",
    "    t = np.sqrt(2 * math.pi * (std**2))\n",
    "    Gaussian = ( lambda x: 1/t * math.exp(-( (x - mu)**2 ) / (2 * (std**2)) ) )\n",
    "    return Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>Race and ethnicity (combined)</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>medcond_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Multiple/Other, Non-Hispanic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0 - 9 Years</td>\n",
       "      <td>Multiple/Other, Non-Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10 - 19 Years</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex      age_group Race and ethnicity (combined)  hosp_yn  icu_yn  \\\n",
       "0    1    0 - 9 Years  Multiple/Other, Non-Hispanic        1       1   \n",
       "1    0    0 - 9 Years               Hispanic/Latino        0       0   \n",
       "2    0    0 - 9 Years  Multiple/Other, Non-Hispanic        0       0   \n",
       "3    0  10 - 19 Years               Hispanic/Latino        1       0   \n",
       "4    1  10 - 19 Years               Hispanic/Latino        0       0   \n",
       "\n",
       "   death_yn  medcond_yn  \n",
       "0         0           1  \n",
       "1         0           0  \n",
       "2         0           0  \n",
       "3         0           1  \n",
       "4         0           0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_small_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_NaiveBayes(data):\n",
    "    pos = {}\n",
    "    neg = {}\n",
    "    P = data.loc[data['death_yn'] == 1].drop(\"death_yn\",axis = 1)\n",
    "    N = data.loc[data['death_yn'] == 0].drop(\"death_yn\",axis = 1)\n",
    "    NB_distribution = {\n",
    "    'sex': Bernoulli_Probability,\n",
    "    'age_group': Multinomial_Probability,\n",
    "    'Race and ethnicity (combined)': Multinomial_Probability,\n",
    "    'hosp_yn': Bernoulli_Probability,\n",
    "    'icu_yn': Bernoulli_Probability,\n",
    "    'medcond_yn': Gaussian_Probability   \n",
    "    }\n",
    "\n",
    "    features = P.keys()\n",
    "\n",
    "    for feature in features:\n",
    "        distribution_func = NB_distribution[feature]\n",
    "        pre_P = P[feature]\n",
    "        pos[feature] = distribution_func(pre_P)\n",
    "        \n",
    "        pre_N = N[feature]\n",
    "        neg[feature] = distribution_func(pre_N)\n",
    "    \n",
    "    return [pos,neg,features]\n",
    "        \n",
    "def predict_with_trained(train_NaiveBayes, personal_feature):\n",
    "    pos = train_NaiveBayes[0]\n",
    "    neg = train_NaiveBayes[1]\n",
    "    features = train_NaiveBayes[2]\n",
    "    P = [pos[i](personal_feature[i]) for i in features]\n",
    "    Q = [neg[i](personal_feature[i]) for i in features]\n",
    "    p = np.prod(P)\n",
    "    q = np.prod(Q)\n",
    "    if p >= q:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def predict(train_NaiveBayes, data):\n",
    "    if data.ndim == 1:\n",
    "        predict(train_NaiveBayes,data)\n",
    "    N = data.shape[0]\n",
    "    return [predict(train_NaiveBayes,data.iloc[i]) for i in range(N)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_fold_Split(data):\n",
    "    \n",
    "    data_len =len(data.index)\n",
    "    data_per_set = int(data_len/10)\n",
    "    result_data = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        start = data_per_set*i\n",
    "        end = data_per_set*(i+1) if i!=9 else data_len\n",
    "        \n",
    "        df1 = data[:start]\n",
    "        df2 = data[end:]\n",
    "        \n",
    "        testing = data[start:end]\n",
    "        training =pd.concat([df1,df2])\n",
    "        \n",
    "        result_data.append([testing,training])\n",
    "        \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_count (test_data, trained_NaiveBayes):\n",
    "    correct_count = 0\n",
    "    test_output = 0\n",
    "    for index, input in test_data.iterrows():\n",
    "        real_output = input[\"death_yn\"]\n",
    "        x = input.drop(\"death_yn\")\n",
    "        test_output = predict_with_trained(trained_NaiveBayes,x)\n",
    "        \n",
    "        if test_output == real_output:\n",
    "            correct_count+=1\n",
    "    return correct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_fold_cross_validation(data):\n",
    "    t_size = len(data)\n",
    "    t_split = ten_fold_Split(data)\n",
    "    correct_total = 0\n",
    "    \n",
    "    for data_entry in t_split:\n",
    "        testing_data = data_entry[0]\n",
    "        training_data = data_entry[1]\n",
    "        trained_NaiveBayes = get_train_NaiveBayes(training_data)\n",
    "        correct_total += correct_count (testing_data, trained_NaiveBayes)\n",
    "\n",
    "    accuracy = correct_total/t_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_NaiveBayes = get_train_NaiveBayes(trim_small_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Bernoulli_Probability.<locals>.<lambda>(x)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_NaiveBayes[0][\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8480247173305943"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_fold_cross_validation(trim_small_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "y_pred=clf.predict(iris.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 ... 2 2 2]\n",
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "2257\n",
      "2257\n",
      "-----\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "-----\n",
      "comp.graphics\n",
      "-----\n",
      "[1 1 3 3 3 3 3 2 2 2]\n",
      "-----\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n",
      "-----\n",
      "  (0, 14887)\t1\n",
      "  (0, 29022)\t1\n",
      "  (0, 8696)\t4\n",
      "  (0, 4017)\t2\n",
      "  (0, 33256)\t2\n",
      "  (0, 21661)\t3\n",
      "  (0, 9031)\t3\n",
      "  (0, 31077)\t1\n",
      "  (0, 9805)\t2\n",
      "  (0, 17366)\t1\n",
      "  (0, 32493)\t4\n",
      "  (0, 16916)\t2\n",
      "  (0, 19780)\t2\n",
      "  (0, 17302)\t2\n",
      "  (0, 23122)\t1\n",
      "  (0, 25663)\t1\n",
      "  (0, 16881)\t1\n",
      "  (0, 16082)\t1\n",
      "  (0, 23915)\t1\n",
      "  (0, 32142)\t5\n",
      "  (0, 33597)\t2\n",
      "  (0, 20253)\t1\n",
      "  (0, 587)\t1\n",
      "  (0, 12051)\t1\n",
      "  (0, 5201)\t1\n",
      "  :\t:\n",
      "  (2256, 13740)\t1\n",
      "  (2256, 14662)\t1\n",
      "  (2256, 20201)\t1\n",
      "  (2256, 12443)\t6\n",
      "  (2256, 30325)\t3\n",
      "  (2256, 4610)\t1\n",
      "  (2256, 33844)\t1\n",
      "  (2256, 17354)\t1\n",
      "  (2256, 26998)\t1\n",
      "  (2256, 20277)\t1\n",
      "  (2256, 20695)\t1\n",
      "  (2256, 20702)\t1\n",
      "  (2256, 9649)\t1\n",
      "  (2256, 9086)\t1\n",
      "  (2256, 26254)\t1\n",
      "  (2256, 17133)\t2\n",
      "  (2256, 4490)\t1\n",
      "  (2256, 13720)\t1\n",
      "  (2256, 5016)\t1\n",
      "  (2256, 9632)\t1\n",
      "  (2256, 11824)\t1\n",
      "  (2256, 29993)\t1\n",
      "  (2256, 1298)\t1\n",
      "  (2256, 2375)\t1\n",
      "  (2256, 3921)\t1\n",
      "(2257, 35788)\n",
      "-----\n",
      "4690\n",
      "-----\n",
      "TfidfTransformer(use_idf=False)\n",
      "-----\n",
      "  (0, 177)\t0.15075567228888181\n",
      "  (0, 230)\t0.07537783614444091\n",
      "  (0, 587)\t0.07537783614444091\n",
      "  (0, 2326)\t0.15075567228888181\n",
      "  (0, 3062)\t0.07537783614444091\n",
      "  (0, 3166)\t0.07537783614444091\n",
      "  (0, 4017)\t0.15075567228888181\n",
      "  (0, 4378)\t0.07537783614444091\n",
      "  (0, 4808)\t0.07537783614444091\n",
      "  (0, 5195)\t0.07537783614444091\n",
      "  (0, 5201)\t0.07537783614444091\n",
      "  (0, 5285)\t0.07537783614444091\n",
      "  (0, 8696)\t0.30151134457776363\n",
      "  (0, 9031)\t0.22613350843332272\n",
      "  (0, 9338)\t0.07537783614444091\n",
      "  (0, 9801)\t0.07537783614444091\n",
      "  (0, 9805)\t0.15075567228888181\n",
      "  (0, 9932)\t0.07537783614444091\n",
      "  (0, 12014)\t0.07537783614444091\n",
      "  (0, 12051)\t0.07537783614444091\n",
      "  (0, 12541)\t0.07537783614444091\n",
      "  (0, 12833)\t0.15075567228888181\n",
      "  (0, 14085)\t0.07537783614444091\n",
      "  (0, 14281)\t0.15075567228888181\n",
      "  (0, 14676)\t0.07537783614444091\n",
      "  :\t:\n",
      "  (2256, 24052)\t0.07216878364870323\n",
      "  (2256, 25560)\t0.07216878364870323\n",
      "  (2256, 26254)\t0.07216878364870323\n",
      "  (2256, 26998)\t0.07216878364870323\n",
      "  (2256, 27031)\t0.14433756729740646\n",
      "  (2256, 27042)\t0.07216878364870323\n",
      "  (2256, 28900)\t0.07216878364870323\n",
      "  (2256, 29224)\t0.07216878364870323\n",
      "  (2256, 29993)\t0.07216878364870323\n",
      "  (2256, 30325)\t0.21650635094610968\n",
      "  (2256, 30498)\t0.07216878364870323\n",
      "  (2256, 30776)\t0.21650635094610968\n",
      "  (2256, 31077)\t0.07216878364870323\n",
      "  (2256, 31342)\t0.07216878364870323\n",
      "  (2256, 31945)\t0.07216878364870323\n",
      "  (2256, 32142)\t0.43301270189221935\n",
      "  (2256, 32233)\t0.07216878364870323\n",
      "  (2256, 32270)\t0.07216878364870323\n",
      "  (2256, 33078)\t0.07216878364870323\n",
      "  (2256, 33844)\t0.07216878364870323\n",
      "  (2256, 34923)\t0.07216878364870323\n",
      "  (2256, 35157)\t0.07216878364870323\n",
      "  (2256, 35350)\t0.07216878364870323\n",
      "  (2256, 35638)\t0.14433756729740646\n",
      "  (2256, 35690)\t0.07216878364870323\n",
      "-----\n",
      "(2257, 35788)\n",
      "-----\n",
      "  (0, 35416)\t0.1348710554299733\n",
      "  (0, 35312)\t0.0312703097833574\n",
      "  (0, 34775)\t0.034481472140846715\n",
      "  (0, 34755)\t0.043341654399042764\n",
      "  (0, 33915)\t0.0999409997803694\n",
      "  (0, 33597)\t0.06567578043186388\n",
      "  (0, 33572)\t0.09313007554599557\n",
      "  (0, 33256)\t0.11819702490105698\n",
      "  (0, 32493)\t0.07283773941616518\n",
      "  (0, 32391)\t0.12806013119559947\n",
      "  (0, 32270)\t0.023871142738151236\n",
      "  (0, 32142)\t0.08865416253721688\n",
      "  (0, 32135)\t0.04910237380446671\n",
      "  (0, 32116)\t0.10218403421141944\n",
      "  (0, 31915)\t0.08631915131162177\n",
      "  (0, 31077)\t0.016797806021219684\n",
      "  (0, 30623)\t0.0686611288079694\n",
      "  (0, 29022)\t0.1348710554299733\n",
      "  (0, 28619)\t0.047271576160535234\n",
      "  (0, 27836)\t0.06899050810672397\n",
      "  (0, 26175)\t0.08497460943470851\n",
      "  (0, 25663)\t0.034290706362898604\n",
      "  (0, 25361)\t0.11947938145690981\n",
      "  (0, 25337)\t0.04935883383975408\n",
      "  (0, 24677)\t0.09796250319482307\n",
      "  :\t:\n",
      "  (2256, 13720)\t0.0969927054646086\n",
      "  (2256, 13521)\t0.06264742916622883\n",
      "  (2256, 13498)\t0.08574361554718753\n",
      "  (2256, 12626)\t0.047531848081675473\n",
      "  (2256, 12443)\t0.5533848656066114\n",
      "  (2256, 11824)\t0.12028503503707108\n",
      "  (2256, 9649)\t0.09916899209319778\n",
      "  (2256, 9632)\t0.11395377721095845\n",
      "  (2256, 9338)\t0.05248982587156077\n",
      "  (2256, 9086)\t0.1056108470261161\n",
      "  (2256, 9072)\t0.06784313233467505\n",
      "  (2256, 7766)\t0.030117682035074988\n",
      "  (2256, 7743)\t0.08847991007710146\n",
      "  (2256, 6507)\t0.26543973023130435\n",
      "  (2256, 6430)\t0.04825278674712813\n",
      "  (2256, 5698)\t0.032261754952242205\n",
      "  (2256, 5529)\t0.031207627784231296\n",
      "  (2256, 5016)\t0.12302132956698503\n",
      "  (2256, 4938)\t0.032031796462786304\n",
      "  (2256, 4610)\t0.0927607242624837\n",
      "  (2256, 4490)\t0.11395377721095845\n",
      "  (2256, 3921)\t0.13532523144219466\n",
      "  (2256, 2375)\t0.12625767908616808\n",
      "  (2256, 1298)\t0.12625767908616808\n",
      "  (2256, 587)\t0.06304633984640515\n",
      "(2257, 35788)\n",
      "-----\n",
      "MultinomialNB()\n",
      "-----\n",
      "[3 1]\n",
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "-----\n",
      "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
      "                ('clf', MultinomialNB())])\n",
      "[3 1]\n",
      "-----\n",
      "0.8348868175765646\n",
      "-----\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-420726b57b90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m text_clf = Pipeline(\n\u001b[1;32m    166\u001b[0m     [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n\u001b[0;32m--> 167\u001b[0;31m      ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;31m# _ = text_clf.fit(twenty_train.data, twenty_train.target)  # 和下面一句的意思一样，一个杠，表示本身\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mtext_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "\"\"\"\n",
    "    这个指南的目的是在一个实际任务上探索scikit-learn的主要工具，在二十个不同的主题上分析一个文本集合。\n",
    "    在这一节中，可以看到：\n",
    "        1、加载文本文件和类别\n",
    "        2、适合机器学习的特征向量提取\n",
    "        3、训练线性模型进行分类\n",
    "        4、使用网格搜索策略，找到一个很好的配置的特征提取组件和分类器\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "    1、Loading the 20 newsgroups dataset 加载20个新闻组数据集\n",
    "    为了获得更快的执行时间为第一个例子，我们将工作在部分数据集只有4个类别的数据集中：\n",
    "\"\"\"\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print(twenty_train.target)\n",
    "print(twenty_train.target_names)  # 训练集中类别的名字，这里只有四个类别\n",
    "print(len(twenty_train.data))  # 训练集中数据的长度\n",
    "print(len(twenty_train.filenames))  # 训练集文件名长度\n",
    "print('-----')\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "print('-----')\n",
    "print(twenty_train.target_names[twenty_train.target[0]])\n",
    "print('-----')\n",
    "print(twenty_train.target[:10])  # 前十个的类别\n",
    "print('-----')\n",
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])  # 类别的名字\n",
    "print('-----')\n",
    "\"\"\"\n",
    "    2、Extracting features from text files 从文本文件中提取特征\n",
    "    为了在文本文件中使用机器学习算法，首先需要将文本内容转换为数值特征向量\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "    Bags of words 词袋\n",
    "    最直接的方式就是词袋表示法\n",
    "        1、为训练集的任何文档中的每个单词分配一个固定的整数ID（例如通过从字典到整型索引建立字典）\n",
    "        2、对于每个文档，计算每个词出现的次数，并存储到X[i,j]中。\n",
    "    词袋表示：n_features 是语料中不同单词的数量，这个数量通常大于100000.\n",
    "    如果 n_samples == 10000，存储X的数组就需要10000*10000*4byte=4GB,这么大的存储在今天的计算机上是不可能实现的。\n",
    "    幸运的是，X中的大多数值都是0，基于这种原因，我们说词袋是典型的高维稀疏数据集，我们可以只存储那些非0的特征向量。\n",
    "    scipy.sparse 矩阵就是这种数据结构，而scikit-learn内置了这种数据结构。\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "    Tokenizing text with scikit-learn 使用scikit-learn标记文本\n",
    "    文本处理、分词、过滤停用词都在这些高级组件中，能够建立特征字典并将文档转换成特征向量。\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # sklearn中的文本特征提取组件中，导入特征向量计数函数\n",
    " \n",
    "count_vect = CountVectorizer()  # 特征向量计数函数\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)  # 对文本进行特征向量处理\n",
    "print(X_train_counts)  # 特征向量和特征标签\n",
    "print(X_train_counts.shape)  # 形状\n",
    "print('-----')\n",
    " \n",
    "\"\"\"\n",
    "    CountVectorizer支持计算单词或序列的N-grams，一旦合适，这个向量化就可以建立特征词典。\n",
    "    在整个训练预料中，词汇中的词汇索引值与其频率有关。\n",
    "\"\"\"\n",
    "print(count_vect.vocabulary_.get(u'algorithm'))\n",
    "print('-----')\n",
    " \n",
    "\"\"\"\n",
    "    From occurrences to frequencies 从事件到频率\n",
    "    计数是一个好的开始，但是也存在一个问题：较长的文本将会比较短的文本有很高的平均计数值，即使他们所表示的话题是一样的。\n",
    "    为了避免潜在的差异，它可以将文档中的每个单词出现的次数在文档的总字数的比例：这个新的特征叫做词频：tf\n",
    "    tf-idf:词频-逆文档频率\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # sklearn中的文本特征提取组件中，导入词频统计函数\n",
    " \n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)  # 建立词频统计函数,注意这里idf=False\n",
    "print(tf_transformer)  # 输出函数属性 TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False, use_idf=False)\n",
    "print('-----')\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)  # 使用函数对文本文档进行tf-idf频率计算\n",
    "print(X_train_tf)\n",
    "print('-----')\n",
    "print(X_train_tf.shape)\n",
    "print('-----')\n",
    "\"\"\"\n",
    "    在上面的例子中，使用fit()方法来构建基于数据的预测器，然后使用transform()方法来将计数矩阵用tf-idf表示。\n",
    "    这两个步骤可以通过跳过冗余处理，来更快的达到相同的最终结果。\n",
    "    这些可以通过使用fit_transform()方法来实现：\n",
    "\"\"\"\n",
    "tfidf_transformer = TfidfTransformer()  # 这里使用的是tf-idf\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf)\n",
    "print(X_train_tfidf.shape)\n",
    "print('-----')\n",
    "\"\"\"\n",
    "    Training a classifier 训练一个分类器\n",
    "    既然已经有了特征，就可以训练分类器来试图预测一个帖子的类别，先使用贝叶斯分类器，贝叶斯分类器提供了一个良好的基线来完成这个任务。\n",
    "    scikit-learn中包括这个分类器的许多变量，最适合进行单词计数的是多项式变量。\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB  # 使用sklearn中的贝叶斯分类器，并且加载贝叶斯分类器\n",
    " \n",
    "# 中的MultinomialNB多项式函数\n",
    "clf = MultinomialNB()  # 加载多项式函数\n",
    "x_clf = clf.fit(X_train_tfidf, twenty_train.target)  # 构造基于数据的分类器\n",
    "print(x_clf)  # 分类器属性：MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "print('-----')\n",
    "\"\"\"\n",
    "    为了预测输入的新的文档，我们需要使用与前面相同的特征提取链进行提取特征。\n",
    "    不同的是，在转换中，使用transform来代替fit_transform，因为训练集已经构造了分类器\n",
    "\"\"\"\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']  # 文档\n",
    "X_new_counts = count_vect.transform(docs_new)  # 构建文档计数\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)  # 构建文档tfidf\n",
    "predicted = clf.predict(X_new_tfidf)  # 预测文档\n",
    "print(predicted)  # 预测类别 [3 1]，一个属于3类，一个属于1类\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))  # 将文档和类别名字对应起来\n",
    "print('-----')\n",
    "\"\"\"\n",
    "    Building a pipeline 建立管道\n",
    "    为了使向量转换更加简单(vectorizer => transformer => classifier)，scikit-learn提供了pipeline类来表示为一个复合分类器\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "print(text_clf)  # 构造分类器，分类器的属性\n",
    "predicted = text_clf.predict(docs_new)  # 预测新文档\n",
    "print(predicted)  # 获取预测值\n",
    "print('-----')\n",
    " \n",
    "\"\"\"\n",
    "    分析总结：\n",
    "        1、加载数据集，主要是加载训练集，用于对数据进行训练\n",
    "        2、文本特征提取：\n",
    "                对文本进行计数统计 CountVectorizer\n",
    "                词频统计  TfidfTransformer  （先计算tf,再计算tfidf）\n",
    "        3、训练分类器：\n",
    "                贝叶斯多项式训练器 MultinomialNB\n",
    "        4、预测文档：\n",
    "                通过构造的训练器进行构造分类器，来进行文档的预测\n",
    "        5、最简单的方式：\n",
    "                通过使用pipeline管道形式，来讲上述所有功能通过管道来一步实现，更加简单的就可以进行预测\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "    Evaluation of the performance on the test set 测试集性能评价\n",
    "    评估模型的预测精度同样容易：\n",
    "\"\"\"\n",
    "import numpy as np\n",
    " \n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print(np.mean(predicted == twenty_test.target))  # 预测的值和测试值的比例，mean就是比例函数\n",
    "print('-----')  # 精度已经为0.834886817577\n",
    " \n",
    "\"\"\"\n",
    "    精度已经实现了83.4%，那么使用支持向量机(SVM)是否能够做的更好呢，支持向量机(SVM)被广泛认为是最好的文本分类算法之一。\n",
    "    尽管，SVM经常比贝叶斯要慢一些。\n",
    "    我们可以改变学习方式，使用管道来实现分类：\n",
    "\"\"\"\n",
    "from sklearn.linear_model import SGDClassifier\n",
    " \n",
    "text_clf = Pipeline(\n",
    "    [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n",
    "# _ = text_clf.fit(twenty_train.data, twenty_train.target)  # 和下面一句的意思一样，一个杠，表示本身\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print(np.mean(predicted == twenty_test.target))  # 精度 0.912782956059\n",
    "print('-----')\n",
    "\"\"\"\n",
    "    sklearn进一步提供了结果的更详细的性能分析工具：\n",
    "\"\"\"\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
